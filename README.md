# PySpark Intelligence Platform

A distributed, observable system for **static analysis, execution graph extraction, lineage tracking, and LLM-assisted explanation of PySpark code**.

This project goes beyond simple code explanation and evolves into a **Spark ETL intelligence layer** capable of understanding transformations, execution stages, data lineage, and performance anti-patterns â€” all exposed through a production-grade architecture.

---

## Overview

This system allows users to submit PySpark code and receive:

- Natural-language explanations (LLM-powered)
- Logical operation DAGs
- Stage-level execution summaries
- Data lineage graphs
- Anti-pattern and performance insights
- Cached and versioned analysis artifacts

The platform is designed with **scalability, observability, and fault tolerance** in mind, separating real-time request handling from heavy analysis workloads.

---

## High-Level Architecture

The system follows a **request â†’ cache â†’ async execution â†’ aggregation** model:

- FastAPI handles validation, orchestration, and status tracking
- Redis provides caching, rate limiting, and job state
- Celery workers execute CPU- and LLM-heavy tasks
- Streamlit provides an interactive UI
- Prometheus and structured logs provide observability

---

## Core Capabilities

- ğŸ” Static analysis of PySpark code via AST parsing
- ğŸ§  Logical DAG construction (transformations & actions)
- ğŸ§¬ Data lineage graph generation
- âš ï¸ Detection of Spark performance anti-patterns
- ğŸ¤– LLM-powered explanations with fallback models
- â™»ï¸ Redis-backed caching for LLM and analysis results
- ğŸ“Š Structured logging and metrics (production-ready)
- ğŸ§µ Asynchronous execution with Celery workers

---

## ğŸ§± Project Structure

```text
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app initialization and lifecycle
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py           # API endpoints (/explain, /status, /health)
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py          # Request/response Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM abstraction (Gemini + fallback logic)
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_pipeline.py     # End-to-end DAG & lineage construction
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py            # Redis helpers (LLM + analysis caching)
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_service_deprecated.py # Legacy DAG service (for reference)
â”‚   â”‚   â”‚   â””â”€â”€ documentation/      # Summarization logic for various components
â”‚   â”‚   â”‚       â”œâ”€â”€ stage_summary.py
â”‚   â”‚   â”‚       â”œâ”€â”€ lineage_summary.py
â”‚   â”‚   â”‚       â”œâ”€â”€ dag_summary.py
â”‚   â”‚   â”‚       â””â”€â”€ antipattern_summary.py
â”‚   â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”‚   â”œâ”€â”€ ast_parser.py       # AST parsing logic
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_semantics.py  # Spark-specific semantics
â”‚   â”‚   â”‚   â””â”€â”€ dag_nodes.py        # DAGNode and ASTNode definitions
â”‚   â”‚   â”œâ”€â”€ graphs/                 # Core graph construction and pattern logic
â”‚   â”‚   â”‚   â”œâ”€â”€ antipatterns/       # Spark performance anti-pattern detection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rules/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ multiple_actions.py
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ repartition_misuse.py
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ action_without_cache.py
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ early_shuffle.py
â”‚   â”‚   â”‚   â”œâ”€â”€ lineage/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ lineage_graph_builder.py
â”‚   â”‚   â”‚   â””â”€â”€ operation/
â”‚   â”‚   â”‚       â”œâ”€â”€ operation_graph_builder.py
â”‚   â”‚   â”‚       â””â”€â”€ stage_assignment.py
â”‚   â”‚   â”œâ”€â”€ visualizers/
â”‚   â”‚   â”‚   â”œâ”€â”€ lineage_graph_visualizer.py   # DOT rendering for lineage
â”‚   â”‚   â”‚   â””â”€â”€ operation_graph_visualizer.py # DOT rendering for operations
â”‚   â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py            # Celery background tasks
â”‚   â”‚   â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_ast_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_dag_visualizer.py
â”‚   â”‚   â”‚   â””â”€â”€ test_dag_builder.py
â”‚   â”‚   â”œâ”€â”€ rate_limit.py           # API rate limiting
â”‚   â”‚   â”œâ”€â”€ config.py               # Environment-based configuration
â”‚   â”‚   â”œâ”€â”€ logging.py              # Centralized logging configuration
â”‚   â”‚   â””â”€â”€ debug_run.py            # Local debugging entry point
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py            # Streamlit UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # Multi-service orchestration
â””â”€â”€ README.md                       # Project-level documentation
```

---

## Running the Project

### Prerequisites

- Docker
- Docker Compose
- Gemini API key

### Environment Configuration

Create a `.env` file in `backend/` with:

- `GEMINI_API_KEY`
- `GEMINI_MODEL`
- `GEMINI_FALLBACK_MODEL`
- `REDIS_URL`

---

### Start the Application

- `docker compose up --build`
- Streamlit UI: http://localhost:8501
- FastAPI backend: http://localhost:8000
- Prometheus metrics: http://localhost:8000/metrics

---

## API Overview

### POST /explain/pyspark

Submits PySpark code for analysis.

- Performs syntax validation
- Checks Redis cache
- Enqueues Celery job if needed

### GET /status/{job_id}

Returns job status and results, including:

- LLM explanation
- DAG and lineage graphs
- Stage summaries
- Anti-pattern detection

---

## Observability

### Logging
- Structured JSON logs
- Correlation via job_id
- Separate logs for API, workers, and cache

### Metrics
- HTTP request rates & latency
- LLM latency and rate-limit events
- Cache hit/miss ratios
- Celery job duration and failures

### Tracing (planned)
- End-to-end request tracing via OpenTelemetry

---

## Technology Stack

| Layer | Tools |
|----|----|
| API | FastAPI, Pydantic |
| Async | Celery |
| Cache & State | Redis |
| Frontend | Streamlit |
| LLM | Gemini (with fallback models) |
| Observability | Structured logs, Prometheus |
| Infra | Docker, Docker Compose |


## Project Roadmap

This project is structured as a multi-stage system that grows into a **Spark ETL intelligence platform**.

### ğŸŸ¦ Stage 1 â€” Core Functionality

- PySpark code submission
- LLM-based explanation
- Structured API responses
- Basic UI

### ğŸŸ© Stage 2 â€” Distributed Architecture

- Redis caching
- Background workers
- Job status API
- Rate limiting
- Fault-tolerant execution

### ğŸŸ§ Stage 3 â€” ETL + Spark Intelligence Layer

- Parse PySpark code into a logical DAG
- Detect transformations and actions
- Identify shuffles and wide dependencies
- Detect performance anti-patterns
- Auto-generate documentation
- Build data lineage graphs

### ğŸŸ¨ Stage 4 â€” System Integration & UX

- Wire DAG + lineage + antipatterns into Celery
- Job lifecycle & status tracking
- Frontend graph rendering
- Streaming results / progressive explanation
- Failure handling
- Versioned analysis artifacts

### ğŸŸ¥ Stage 5 â€” Production Deployment

- Production Docker builds
- Structured logging
- Prometheus metrics
- OpenTelemetry tracing
- CI/CD pipelines
- Deployment-ready configuration

---

## Future Improvements

- Visual DAG rendering
- Multi-file project analysis
- Version comparison
- Interactive lineage graphs
- Performance recommendations

---

## ğŸ“œ License

MIT