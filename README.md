# PySpark Intelligence Platform

A distributed, observable system for **static analysis, execution graph extraction, lineage tracking, and LLM-assisted explanation of PySpark code**.

This project goes beyond simple code explanation and evolves into a **Spark ETL intelligence layer** capable of understanding transformations, execution stages, data lineage, and performance anti-patterns â€” all exposed through a production-grade architecture.

---

## Overview

This system allows users to submit PySpark code and receive:

- Natural-language explanations (LLM-powered)
- Logical operation DAGs
- Stage-level execution summaries
- Data lineage graphs
- Anti-pattern and performance insights
- Cached and versioned analysis artifacts

The platform is designed with **scalability, observability, and fault tolerance** in mind, separating real-time request handling from heavy analysis workloads.

---

## Why this project matters

This project demonstrates my ability to:
- Design distributed, observable backend systems
- Analyze non-trivial code semantics (AST â†’ DAG â†’ lineage)
- Build production-grade async pipelines
- Apply CI, testing, and operational best practices

---

## Key Design Decisions

- **Async via Celery**: Decouples API latency from heavy analysis workloads
- **Redis as shared state**: Enables caching, rate limiting, and job tracking
- **Operation DAG abstraction**: Separates logical Spark semantics from visualization
- **Structured logs + metrics**: Enables post-mortem debugging and capacity planning


---

## High-Level Architecture

The system follows a **request â†’ cache â†’ async execution â†’ aggregation** model:

- FastAPI handles validation, orchestration, and status tracking
- Redis provides caching, rate limiting, and job state
- Celery workers execute CPU- and LLM-heavy tasks
- Streamlit provides an interactive UI
- Prometheus and structured logs provide observability

---

## ðŸ§± Project Structure

```text
.
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py                 # FastAPI app initialization and lifecycle
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py           # API endpoints (/explain, /status, /health)
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py          # Request/response Pydantic models
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ llm.py              # LLM abstraction (Gemini + fallback logic)
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_pipeline.py     # End-to-end DAG & lineage construction
â”‚   â”‚   â”‚   â”œâ”€â”€ cache.py            # Redis helpers (LLM + analysis caching)
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_service_deprecated.py # Legacy DAG service (for reference)
â”‚   â”‚   â”‚   â””â”€â”€ documentation/      # Summarization logic for various components
â”‚   â”‚   â”‚       â”œâ”€â”€ stage_summary.py
â”‚   â”‚   â”‚       â”œâ”€â”€ lineage_summary.py
â”‚   â”‚   â”‚       â”œâ”€â”€ dag_summary.py
â”‚   â”‚   â”‚       â””â”€â”€ antipattern_summary.py
â”‚   â”‚   â”œâ”€â”€ parsers/
â”‚   â”‚   â”‚   â”œâ”€â”€ ast_parser.py       # AST parsing logic
â”‚   â”‚   â”‚   â”œâ”€â”€ spark_semantics.py  # Spark-specific semantics
â”‚   â”‚   â”‚   â””â”€â”€ dag_nodes.py        # DAGNode and ASTNode definitions
â”‚   â”‚   â”œâ”€â”€ graphs/                 # Core graph construction and pattern logic
â”‚   â”‚   â”‚   â”œâ”€â”€ antipatterns/       # Spark performance anti-pattern detection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ registry.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ rules/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ multiple_actions.py
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ repartition_misuse.py
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ action_without_cache.py
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ early_shuffle.py
â”‚   â”‚   â”‚   â”œâ”€â”€ lineage/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ lineage_graph_builder.py
â”‚   â”‚   â”‚   â””â”€â”€ operation/
â”‚   â”‚   â”‚       â”œâ”€â”€ operation_graph_builder.py
â”‚   â”‚   â”‚       â””â”€â”€ stage_assignment.py
â”‚   â”‚   â”œâ”€â”€ visualizers/
â”‚   â”‚   â”‚   â”œâ”€â”€ lineage_graph_visualizer.py   # DOT rendering for lineage
â”‚   â”‚   â”‚   â””â”€â”€ operation_graph_visualizer.py # DOT rendering for operations
â”‚   â”‚   â”œâ”€â”€ workers/
â”‚   â”‚   â”‚   â””â”€â”€ tasks.py            # Celery background tasks
â”‚   â”‚   â”œâ”€â”€ tests/                  # Unit and integration tests
â”‚   â”‚   â”‚   â”œâ”€â”€ test_ast_parser.py
â”‚   â”‚   â”‚   â”œâ”€â”€ test_dag_visualizer.py
â”‚   â”‚   â”‚   â””â”€â”€ test_dag_builder.py
â”‚   â”‚   â”œâ”€â”€ rate_limit.py           # API rate limiting
â”‚   â”‚   â”œâ”€â”€ config.py               # Environment-based configuration
â”‚   â”‚   â”œâ”€â”€ logging.py              # Centralized logging configuration
â”‚   â”‚   â””â”€â”€ debug_run.py            # Local debugging entry point
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ streamlit_app.py            # Streamlit UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ docker-compose.yml              # Multi-service orchestration
â””â”€â”€ README.md                       # Project-level documentation
```

---

## Running the Project

### Prerequisites

- Docker
- Docker Compose
- Gemini API key

### Environment Configuration

Follow the `.env.example` to create a `.env` file with your variables values.

---

### Start the Application

- `docker compose up --build`
- Streamlit UI: http://localhost:8501
- FastAPI backend: http://localhost:8000
- Prometheus metrics: http://localhost:8000/metrics
- Jaeger UI: http://localhost:16686

---

## Observability

### Logging
- Structured JSON logs
- Correlation via job_id
- Separate logs for API, workers, and cache

### Metrics
- HTTP request rates & latency
- LLM latency and rate-limit events
- Cache hit/miss ratios
- Celery job duration and failures

### Tracing (planned)
- End-to-end request tracing via OpenTelemetry

---

## Project Roadmap

This project is structured as a multi-stage system that grows into a **Spark ETL intelligence platform**.

### ðŸŸ¦ Stage 1 â€” Core Functionality

- PySpark code submission
- LLM-based explanation
- Structured API responses
- Basic UI

### ðŸŸ© Stage 2 â€” Distributed Architecture

- Redis caching
- Background workers
- Job status API
- Rate limiting
- Fault-tolerant execution

### ðŸŸ§ Stage 3 â€” ETL + Spark Intelligence Layer

- Parse PySpark code into a logical DAG
- Detect transformations and actions
- Identify shuffles and wide dependencies
- Detect performance anti-patterns
- Auto-generate documentation
- Build data lineage graphs

### ðŸŸ¨ Stage 4 â€” System Integration & UX

- Wire DAG + lineage + antipatterns into Celery
- Job lifecycle & status tracking
- Frontend graph rendering
- Streaming results / progressive explanation
- Failure handling
- Versioned analysis artifacts

### ðŸŸ¥ Stage 5 â€” Production Deployment

- Production Docker builds
- Structured logging
- Prometheus metrics
- OpenTelemetry tracing
- CI/CD pipelines
- Deployment-ready configuration

---

## Future Improvements

- Visual DAG rendering
- Multi-file project analysis
- Version comparison
- Interactive lineage graphs
- Performance recommendations

---

## ðŸ“œ License

MIT